{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyM6iuSaQ6bNG7QG8KyV0zmZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chaos-woo/qwen3-ft-template/blob/main/%E5%BE%AE%E8%B0%83Qwen3_0_6b%E5%AE%9E%E6%88%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "环境安装"
      ],
      "metadata": {
        "id": "bpnmQRwvF_-Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O3J1vm_PFzi2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install transformers>=4.33.0\n",
        "    !pip install bitsandbytes accelerate xformers==0.0.29.post3 peft==0.14.0 trl==0.15.2 triton cut_cross_entropy\n",
        "    !pip install sentencepiece protobuf==3.20.3 datasets huggingface_hub hf_transfer\n",
        "    !pip install unsloth_zoo>=2025.3.7\n",
        "    !pip install --upgrade --no-cache-dir unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "引入Unsloth框架，导入模型和分词器"
      ],
      "metadata": {
        "id": "eIFu5UYgIlVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n",
        "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
        "\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Phi-4\",\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-0.6B\",\n",
        "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
        "    load_in_4bit = True,     # 4bit uses much less memory\n",
        "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # We have full finetuning now!\n",
        "    # token = \"hf_...\",      # use one if using gated models\n",
        ")"
      ],
      "metadata": {
        "id": "CEludZxnIsG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "设置模型参数"
      ],
      "metadata": {
        "id": "fOE-URPAWjzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,   # We support rank stabilized LoRA\n",
        "    loftq_config = None,  # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "id": "wTo6-pAMWm0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "从Hugging Face拉取数据集"
      ],
      "metadata": {
        "id": "I1PrylU1WozG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "non_reasoning_dataset = load_dataset(\"ch1so/ruozhiba_qa\", split = \"train[:100]\") #取前100数据"
      ],
      "metadata": {
        "id": "I1nXm-GMWrWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "查看数据集结构"
      ],
      "metadata": {
        "id": "1bbbpdyVXKo1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_reasoning_dataset"
      ],
      "metadata": {
        "id": "hYgCHwmNXMi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "查看数据集第一行数据的结构"
      ],
      "metadata": {
        "id": "r31dOWnLXN89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_reasoning_dataset[0]"
      ],
      "metadata": {
        "id": "QVpT-62kXPmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "将数据集修改为ShareGPT格式"
      ],
      "metadata": {
        "id": "JAvQ1sVUXV0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from unsloth.chat_templates import standardize_sharegpt\n",
        "\n",
        "# 将原始JSON转换为对话格式列表，便于后续模板化\n",
        "conversations = []\n",
        "for item in non_reasoning_dataset:\n",
        "    conversations.append([\n",
        "        {\"role\": \"user\", \"content\": item[\"instruction\"]},\n",
        "        {\"role\": \"assistant\", \"content\": item[\"output\"]},\n",
        "    ])\n",
        "# 将list转成Dataset\n",
        "raw_conversations_ds = Dataset.from_dict({\"conversations\": conversations})\n",
        "dataset = standardize_sharegpt(raw_conversations_ds)\n",
        "\n",
        "non_reasoning_conversations = tokenizer.apply_chat_template(\n",
        "    dataset[\"conversations\"],\n",
        "    tokenize = False,\n",
        ")"
      ],
      "metadata": {
        "id": "x0U2vgZnXXqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "查看转换后的数据结构"
      ],
      "metadata": {
        "id": "VurZ76vkXZYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_reasoning_conversations[0]"
      ],
      "metadata": {
        "id": "DkgCbZxBXawj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "将数据进行合并或混合，可以加载多个目标任务不相同的数据集，然后将数据集按照比例混合，进行混合训练，可以做到多种目标任务同时进行，并且因为是有比例的混合，减少先后训练导致的后训练任务过拟合，这里仅作为测试，所有仅使用一种数据集转换为混合数据集\n",
        "\n",
        "加载COT数据集的1种方法\n",
        "```\n",
        "reasoning_dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split = \"cot\")\n",
        "def generate_conversation(examples):\n",
        "    problems  = examples[\"problem\"]\n",
        "    solutions = examples[\"generated_solution\"]\n",
        "    conversations = []\n",
        "    for problem, solution in zip(problems, solutions):\n",
        "        conversations.append([\n",
        "            {\"role\" : \"user\",      \"content\" : problem},\n",
        "            {\"role\" : \"assistant\", \"content\" : solution},\n",
        "        ])\n",
        "    return { \"conversations\": conversations, }\n",
        "reasoning_conversations = tokenizer.apply_chat_template(\n",
        "    reasoning_dataset.map(generate_conversation, batched = True)[\"conversations\"],\n",
        "    tokenize = False,\n",
        ")\n",
        "```\n",
        "混合两种数据集的1种方法\n",
        "```\n",
        "import pandas as pd\n",
        "non_reasoning_subset = pd.Series(non_reasoning_conversations)\n",
        "non_reasoning_subset = non_reasoning_subset.sample(\n",
        "    int(len(reasoning_conversations) * (1.0 - chat_percentage)),\n",
        "    random_state = 2407,\n",
        ")\n",
        "data = pd.concat([\n",
        "    pd.Series(reasoning_conversations),\n",
        "    pd.Series(non_reasoning_subset)\n",
        "])\n",
        "data.name = \"text\"\n",
        "\n",
        "from datasets import Dataset\n",
        "combined_dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
        "combined_dataset = combined_dataset.shuffle(seed = 3407)\n",
        "```"
      ],
      "metadata": {
        "id": "difs424MXdVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "combined_dataset = Dataset.from_pandas(pd.DataFrame(pd.Series(non_reasoning_conversations)))\n",
        "combined_dataset = combined_dataset.shuffle(seed = 3407)"
      ],
      "metadata": {
        "id": "9RSAiJIaXfjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "配置训练配置"
      ],
      "metadata": {
        "id": "2bu7lAl_XnhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "def formatting_func(example):\n",
        "    # text = (f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "    #        f\"{example['question']}<|eot_id|><|start_header_id|>\"\n",
        "    #        f\"assistant<|end_header_id|>\\n\\n{example['answer']}<|eot_id|>\")\n",
        "    text = example['0']\n",
        "    if isinstance(text, list):  # 如果 text 是列表\n",
        "        return text  # 直接返回列表\n",
        "    elif isinstance(text, str):  # 如果 text 是字符串\n",
        "        return [text]  # 返回包含单个字符串的列表\n",
        "    else:\n",
        "        raise ValueError(f\"Unexpected type for example['0']: {type(text)}\")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = combined_dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    formatting_func = formatting_func,\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        max_steps = 30,\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "MNkCupf1XpOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "查看下当前的资源"
      ],
      "metadata": {
        "id": "owEv3SijXr0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "IYyL3HorXtko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "开始训练"
      ],
      "metadata": {
        "id": "OWsaS1WbXvGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "eer1jOxYXwZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "训练完成后再查看一下资源"
      ],
      "metadata": {
        "id": "VWOQo-2jXyBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ],
      "metadata": {
        "id": "uXyCPhQaXzai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "模型对话效果测试"
      ],
      "metadata": {
        "id": "C33bW3bMX1M5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\" : \"user\", \"content\" : \"爸爸再婚，我是不是就有了个新娘？\"}\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    enable_thinking = False, # Disable thinking\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    max_new_tokens = 256, # Increase for longer outputs!\n",
        "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
        ")"
      ],
      "metadata": {
        "id": "ElXGeEiRX2h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "推送到Hugging Face"
      ],
      "metadata": {
        "id": "hqUYjhLuX4EQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model.save_pretrained(\"lora_model\")  # Local saving\n",
        "#tokenizer.save_pretrained(\"lora_model\")\n",
        "model.push_to_hub(\"ch1so/qwen3-0.6b-ruozhiba-100\", token = \"hfxxx\") # Online saving\n",
        "tokenizer.push_to_hub(\"ch1so/qwen3-0.6b-ruozhiba-100\", token = \"hfxxx\") # Online saving"
      ],
      "metadata": {
        "id": "As8B2JtHX50n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "加载LoRA适配器，把我们刚才微调好的模型加载到环境中"
      ],
      "metadata": {
        "id": "jpq8KcECX7ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"ch1so/qwen3-0.6b-ruozhiba-100\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = 2048,\n",
        "        load_in_4bit = True,\n",
        "    )"
      ],
      "metadata": {
        "id": "jVmqrlQ1X88X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "将我们微调好的模型量化为gguf格式，再推送到Hugging Face上"
      ],
      "metadata": {
        "id": "QbH3FwUKX-k9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False:\n",
        "    model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: # Pushing to HF Hub\n",
        "    model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if True:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"ch1so/qwen3-0.6b-ruozhiba-100-gguf\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"f16\", \"q4_k_m\", \"q8_0\", \"q5_k_m\"],\n",
        "        token = \"hfXXX\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ],
      "metadata": {
        "id": "9YGx3Cl2YAjy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}